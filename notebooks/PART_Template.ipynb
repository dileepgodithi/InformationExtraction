{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/meghanasolleti/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/meghanasolleti/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import spacy\n",
    "import networkx as nx\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nlp = spacy.load('en')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from spacy import displacy\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_part_relations(file_name):\n",
    "    with open(\"/Users/meghanasolleti/Documents/NLP/Project/WikipediaArticles/\"+file_name,'r',encoding='utf-8-sig',errors=\"ignore\") as file:\n",
    "        data=file.read()\n",
    "        sentences=sent_tokenize(data)\n",
    "    json_template = {\"document\": file_name, \"extraction\":[]}\n",
    "    for sentence in sentences:\n",
    "        part=[]\n",
    "        doc = nlp(sentence)\n",
    "        edges = []\n",
    "        e_list={}\n",
    "        nodes=[]\n",
    "        tokens=[]\n",
    "        part_match=\"False\"\n",
    "    \n",
    "        for ent in doc.ents:\n",
    "            if(ent.text==\"Richardson\"):\n",
    "                e_list[ent.text]='GPE'\n",
    "                nodes=nodes+[ent.text]\n",
    "            if(ent.label_=='GPE' and e_list.get(ent.text) is None):\n",
    "                e_list[ent.text]=ent.label_\n",
    "                nodes=nodes+[ent.text]\n",
    "            \n",
    "        for token in doc:\n",
    "            for child in token.children:\n",
    "                if(token.text not in tokens):        \n",
    "                    tokens=tokens+[token.text]\n",
    "                edges.append(('{0}'.format(token.lower_),\n",
    "                      '{0}'.format(child.lower_)))\n",
    "        graph = nx.Graph(edges)\n",
    "        digraph=nx.DiGraph(edges)\n",
    "        for i in range(len(nodes)):\n",
    "            for j in range(i+1,len(nodes)):\n",
    "                entity1=nodes[i].lower()\n",
    "                entity2=nodes[j].lower()\n",
    "                if(('is' in tokens) and (nx.has_path(digraph, source='is', target=entity1)) and (nx.has_path(digraph, source='is', target=entity2))):\n",
    "                    s=(entity1,entity2)\n",
    "                    if s not in part:\n",
    "                        part=part+[s]\n",
    "                if(('are' in tokens) and (nx.has_path(digraph, source='are', target=entity1)) and (nx.has_path(digraph, source='are', target=entity2))):\n",
    "                    s=(entity1,entity2)\n",
    "                    if s not in part:\n",
    "                        part=part+[s]\n",
    "                if(('in' in tokens) and (nx.has_path(digraph, source='in', target=entity1)) and (nx.has_path(digraph, source='in', target=entity2))):\n",
    "                    s=(entity1,entity2)\n",
    "                    if s not in part:\n",
    "                        part=part+[s]\n",
    "                if((nx.has_path(graph, source=entity1, target=entity2))):\n",
    "                    nodes_in_path=nx.shortest_path(graph, source=entity1, target=entity2)\n",
    "                    if('is' in nodes_in_path and 'in' in nodes_in_path):\n",
    "                        s=(entity1,entity2)\n",
    "                        if s not in part:\n",
    "                            part=part+[s]\n",
    "                    if('is' in nodes_in_path and 'of' in nodes_in_path):\n",
    "                        s=(entity1,entity2)\n",
    "                        if s not in part:\n",
    "                            part=part+[s]\n",
    "                    if('in' in nodes_in_path and 'of' in nodes_in_path):\n",
    "                        s=(entity1,entity2)\n",
    "                        if s not in part:\n",
    "                            part=part+[s]\n",
    "                    if('in' in nodes_in_path and 'of' not in nodes_in_path):\n",
    "                        s=(entity1,entity2)\n",
    "                        if s not in part:\n",
    "                            part=part+[s]\n",
    "                    if('in' not in nodes_in_path and 'is' not in nodes_in_path and 'by' not in nodes_in_path and 'of' in nodes_in_path):\n",
    "                        s=(entity1,entity2)\n",
    "                        if s not in part:\n",
    "                            part=part+[s]\n",
    "                    if('are' in nodes_in_path ):\n",
    "                        s=(entity1,entity2)\n",
    "                        if s not in part:\n",
    "                            part=part+[s]\n",
    "                    if('among' in nodes_in_path):\n",
    "                        s=(entity1,entity2)\n",
    "                        if s not in part:\n",
    "                            part=part+[s] \n",
    "        if(len(part)!=0):\n",
    "            part_match=\"true\"\n",
    "        if(part_match):\n",
    "            for i in range(len(part)):\n",
    "                relation_dict = {\"template\": \"PART\", \"sentences\": [], \"arguments\": {\"1\": \"\", \"2\": \"\"}}\n",
    "                relation_dict[\"sentences\"].append(sentence)\n",
    "                relation_dict[\"arguments\"][\"1\"] = part[i][0]\n",
    "                relation_dict[\"arguments\"][\"2\"] = part[i][1]\n",
    "                json_template[\"extraction\"].append(relation_dict)\n",
    "    return(json_template)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    arg_list = sys.argv\n",
    "    path =\"/Users/meghanasolleti/Documents/NLP/Project/WikipediaArticles/\"\n",
    "    #directory path\n",
    "    merge_ents = nlp.create_pipe(\"merge_entities\")\n",
    "    nlp.add_pipe(merge_ents)\n",
    "    with os.scandir(path) as directory:\n",
    "        for entry in directory:\n",
    "            file_name = entry.name\n",
    "#             print(file_name)\n",
    "            json_part = show_part_relations(file_name)\n",
    "            json_file = file_name[:len(file_name)-4] + \".json\"\n",
    "            json_object = json.loads(json.dumps(json_part))\n",
    "            json_formatted_str = json.dumps(json_object, indent=4)\n",
    "            file = open('JSON_PART/'+json_file, \"a+\")\n",
    "            n = file.write(json_formatted_str)\n",
    "            file.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
