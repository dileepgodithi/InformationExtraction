{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import neuralcoref\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_nps = nlp.create_pipe(\"merge_noun_chunks\")\n",
    "# nlp.add_pipe(merge_nps)\n",
    "# merge_ents = nlp.create_pipe(\"merge_entities\")\n",
    "# nlp.add_pipe(merge_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_spans(spans):\n",
    "    # Filter a sequence of spans so they don't contain overlaps\n",
    "    # For spaCy 2.1.4+: this function is available as spacy.util.filter_spans()\n",
    "    get_sort_key = lambda span: (span.end - span.start, -span.start)\n",
    "    sorted_spans = sorted(spans, key=get_sort_key, reverse=True)\n",
    "    result = []\n",
    "    seen_tokens = set()\n",
    "    for span in sorted_spans:\n",
    "        # Check for end - 1 here because boundaries are inclusive\n",
    "        if span.start not in seen_tokens and span.end - 1 not in seen_tokens:\n",
    "            result.append(span)\n",
    "        seen_tokens.update(range(span.start, span.end))\n",
    "    result = sorted(result, key=lambda span: span.start)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_object(obj, r_subtree):\n",
    "    if obj.n_rights:\n",
    "        r_subtree += list(obj.rights)\n",
    "        for child in obj.rights:\n",
    "            get_final_object(child, r_subtree)\n",
    "    return r_subtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_subject(sub, l_subtree):\n",
    "    if sub.n_lefts:\n",
    "        l_subtree += list(sub.lefts)\n",
    "        for child in sub.lefts:\n",
    "            get_final_subject(child, l_subtree)\n",
    "    return l_subtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source(token):\n",
    "    source = None\n",
    "    for child in token.rights:\n",
    "        if child.text == 'of' or child.text == 'in':\n",
    "            if child.right_edge.ent_type_ != 'GPE' and child.right_edge.pos_ in ('PROPN','NOUN'):\n",
    "                source = child.right_edge\n",
    "        if child.text == 'from':\n",
    "            source = child.right_edge\n",
    "    return source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_coref(token):\n",
    "    try:\n",
    "        if token._.in_coref:\n",
    "            token = token._.coref_clusters[0][0]\n",
    "    finally:\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_buy_relations(doc, file_name): \n",
    "    #doc = doc._.coref_resolved\n",
    "    sents = list(doc.sents)\n",
    "    buy_relations = []\n",
    "    #relation_dict = {\"template\": \"BUY\", \"sentences\": [], \"arguments\": {\"1\": \"\", \"2\": \"\", \"3\": \"\",\"4\": \"\",\"5\": \"\"}\n",
    "    json_template = {\"document\": file_name, \"extraction\":[]}\n",
    "    for sent in sents:\n",
    "        try:\n",
    "            spans = list(sent.ents) + list(sent.noun_chunks)\n",
    "            spans = filter_spans(spans)\n",
    "            with doc.retokenize() as retokenizer:\n",
    "                for span in spans:\n",
    "                    retokenizer.merge(span)\n",
    "            final_subject = None\n",
    "            final_object = ''\n",
    "            source = ''\n",
    "            amount = ''\n",
    "            buy_match = False\n",
    "            relation = ''\n",
    "            main_subject = None\n",
    "            quantity = ''\n",
    "            relation_dict = {\"template\": \"BUY\", \"sentences\": [], \"arguments\": {\"1\": \"\", \"2\": \"\", \"3\": \"\",\"4\": \"\",\"5\": \"\"}}\n",
    "            for token in sent:\n",
    "                if token.ent_type_ == \"MONEY\":\n",
    "                    amount = token.text\n",
    "                    i = 1\n",
    "                    while token.nbor(-i).pos_ in ('SYM','NUM'):\n",
    "                        amount = token.nbor(-i).text + ' ' + amount\n",
    "                        i += 1\n",
    "                if token.dep_ == \"nsubj\" and main_subject is None:\n",
    "                    main_subject = token\n",
    "                if token.lemma_ == 'buy' or token.lemma_ == 'acquire' or token.lemma_ == 'purchase':\n",
    "                    buy_match = True\n",
    "                    relation = token\n",
    "                    subject = [w for w in token.lefts if w.dep_ == \"nsubj\"]\n",
    "                    obj = [w for w in token.rights if w.dep_ == \"dobj\"]\n",
    "                    if subject:\n",
    "                        final_subject = subject[0]\n",
    "                    if obj:\n",
    "                        final_object = obj[0]\n",
    "                        r_subtree = get_final_object(obj[0], [])\n",
    "                        source = get_source(token)\n",
    "                        for child in r_subtree:\n",
    "                            #print(r_subtree,child,child.ent_type_)\n",
    "                            if child.text == 'of'  or child.text == 'from' or child.text == 'in' :\n",
    "                                if child.ent_type_ != 'GPE' and source is None:\n",
    "                                    source = child.right_edge\n",
    "                            if child.ent_type_ == \"MONEY\":\n",
    "                                amount = child  \n",
    "                            if child.ent_type_ == \"QUANTITY\":\n",
    "                                quantity = child  \n",
    "                        if final_object.n_rights:\n",
    "    #                         edge.pos_ == 'PROPN':\n",
    "    #                         final_object = final_object.right_edge   \n",
    "                            right_PN = [w for w in final_object.rights if w.pos_ == \"PROPN\"]\n",
    "                            if right_PN:\n",
    "                                final_object = right_PN[0]\n",
    "                    if not subject and not obj:\n",
    "                        obj = [w for w in token.lefts if w.dep_ == \"nsubjpass\"]\n",
    "                        subject = [w for w in token.subtree if w.dep_ == \"pobj\" and w.nbor(-1).text == 'by']\n",
    "                        final_object = obj[0] if obj else ''\n",
    "                        final_subject = subject[0] if subject else None   \n",
    "                    if final_subject is None:\n",
    "                        l_subtree = get_final_subject(token.head, [])\n",
    "                        #print(l_subtree)\n",
    "                        final_subject = [w for w in l_subtree if w.dep_ == 'nsubj']\n",
    "                        final_subject = final_subject[0] if final_subject else main_subject\n",
    "                #if token.ent_type\n",
    "            if buy_match:\n",
    "    #             if final_subject and final_subject.pos_ == 'PRON':\n",
    "    #                 print(final_subject)\n",
    "    #                 try:\n",
    "    #                     if final_subject._.in_coref:\n",
    "    #                         print(final_subject._.coref_clusters)\n",
    "    #                         final_subject = final_subject._.coref_clusters[0][0]\n",
    "    #                         print(final_subject) \n",
    "    #                 except:\n",
    "    #                     print('exception')\n",
    "    #             else:\n",
    "    #                 print('no subject')\n",
    "                buy_relations.append({(final_subject, relation, final_object, source, amount) : sent})\n",
    "                relation_dict[\"sentences\"].append(sent.text)\n",
    "                relation_dict[\"arguments\"][\"1\"] = final_subject.text if final_subject else \"\"\n",
    "                relation_dict[\"arguments\"][\"2\"] = final_object.text if final_object else \"\"\n",
    "                relation_dict[\"arguments\"][\"3\"] = amount if amount else \"\"\n",
    "                relation_dict[\"arguments\"][\"4\"] = quantity.text if quantity else \"\"\n",
    "                relation_dict[\"arguments\"][\"5\"] = source.text if source else \"\"\n",
    "                json_template[\"extraction\"].append(relation_dict)\n",
    "        except:\n",
    "            continue\n",
    "                #displacy.render(sent, style='dep', jupyter=True)\n",
    "                #print(buy_relations[-1])\n",
    "    #print(json_template)\n",
    "    #return buy_relations\n",
    "    return json_template\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_nlpdoc_object(file_name):\n",
    "    #relations = None\n",
    "    with open('WikipediaArticles/'+file_name, 'r', encoding='utf-8',errors=\"ignore\") as file:\n",
    "        content = file.read()\n",
    "        doc = nlp(content)\n",
    "        #relations = show_buy_relations(doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "with os.scandir('WikipediaArticles/') as directory:\n",
    "    for entry in directory:\n",
    "        #print(entry.name)\n",
    "        file_name = entry.name\n",
    "        doc = create_nlpdoc_object(file_name)\n",
    "        #doc = nlp(doc._.coref_resolved)\n",
    "        json_buy = show_buy_relations(doc, file_name)\n",
    "        \n",
    "        json_file = file_name[:len(file_name)-4] + \".json\"\n",
    "        json_object = json.loads(json.dumps(json_buy))\n",
    "        json_formatted_str = json.dumps(json_object, indent=4)\n",
    "        file = open('JSON/'+json_file, \"a+\")\n",
    "        n = file.write(json_formatted_str)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
